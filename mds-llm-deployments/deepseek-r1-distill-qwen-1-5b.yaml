# Working version of deployment without 
apiVersion: apps/v1
kind: Deployment
metadata:
  name: deepseek-r1-distill-qwen-1-5b
  labels:
    app: deepseek-r1-distill-qwen-1-5b
spec:
  replicas: 1  # Adjust based on your needs
  selector:
    matchLabels:
      app: deepseek-r1-distill-qwen-1-5b
  template:
    metadata:
      labels:
        app: deepseek-r1-distill-qwen-1-5b
    spec:
      initContainers:
      - name: model-downloader
        image: alpine:3.18
        command: ["/bin/sh", "-c"]
        args:
        - |
          apk add --no-cache curl git git-lfs openssh && \
          git lfs install && \
          git clone https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B && \
          git clone --filter=blob:none --no-checkout https://github.com/khutchi2/llm_configs.git && \
          cd llm_configs && \
          git sparse-checkout init --cone && \
          git sparse-checkout set vllm_models_deepseek1-5b && \
          git checkout main && \
          cp -r vllm_models_deepseek1-5b /models/
        volumeMounts:
          - mountPath: /models
            name: model-storage
      containers:
      - name: triton
        image: 670742214767.dkr.ecr.us-west-2.amazonaws.com/triton_vllm_openai:24.08
        command: ["/usr/bin/python3", "/workspace/server/python/openai/openai_frontend/main.py"]
        args:
        - --model-repository=$(SERVER_MODELS_DIR)
        - --kserve-http-port=$(SERVER_HTTP_PORT)
        - --kserve-grpc-port=$(SERVER_GRPC_PORT)
        - --tokenizer=deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B  # Name as it appears on HF -- <organization/model>
        resources:
          limits:
            nvidia.com/gpu: "1"
          requests:
            nvidia.com/gpu: "1"
        volumeMounts:
          - mountPath: /models
            name: model-storage
            # hostPath:
            #   path: /models
            #   type: DirectoryOrCreate
          # - mountPath: /var/run/models
          #   name: model-repository
          - mountPath: /dev/shm
            name: dshm
        ports:
        - containerPort: 9500
          name: server-grpc
          protocol: TCP
        - containerPort: 9000
          name: server-http
          protocol: TCP
        - containerPort: 8002
          name: server-metrics
          protocol: TCP
        env:
          - name: HF_HOME
            value: /models/.cache
          - name: HF_TOKEN
            value: <insert_HF_token>
          - name: SERVER_HTTP_PORT
            value: "9000"
          - name: SERVER_GRPC_PORT
            value: "9500"
          - name: SERVER_MODELS_DIR
            value: /models/vllm_models_deepseek1-5b
          - name: LD_PRELOAD
            value: /usr/lib/x86_64-linux-gnu/libtcmalloc.so.4
      volumes:
        - name: model-storage
          hostPath:
            path: /models
            type: DirectoryOrCreate
        # - name: model-repository
        #   persistentVolumeClaim:
        #     claimName: efs-claim
        - name: dshm
          emptyDir:
            medium: Memory
            sizeLimit: 32Gi
      nodeSelector:
        node.kubernetes.io/instance-type: g5.xlarge
      tolerations:
        - key: "nvidia.com/gpu.present"
          operator: "Equal"
          value: "true"
          effect: "NoSchedule"
        - key: "gpu"
          operator: "Equal"
          value: "true"
          effect: "NoSchedule"

---

apiVersion: v1
kind: Service
metadata:
  name: deepseek-r1-distill-qwen-1-5b
  namespace: seldon-mesh
  labels:
    app: triton-vllm
spec:
  externalTrafficPolicy: Cluster
  internalTrafficPolicy: Cluster
  ipFamilies:
  - IPv4
  ipFamilyPolicy: SingleStack
  ports:
  - name: http
    port: 9000
    protocol: TCP
    targetPort: server-http
  - name: grpc
    port: 9500
    protocol: TCP
    targetPort: server-grpc
  selector:
    app: deepseek-r1-distill-qwen-1-5b
  sessionAffinity: None
  type: NodePort
status:
  loadBalancer: {}

---

# apiVersion: keda.sh/v1alpha1
# kind: ScaledObject
# metadata:
#   name: deepseek-r1-distill-qwen-1-5b
# spec:
#   scaleTargetRef:
#     kind: Deployment
#     name: deepseek-r1-distill-qwen-1-5b
#   pollingInterval:  30 
#   cooldownPeriod:   300
#   minReplicaCount:  0
#   maxReplicaCount:  1
#   triggers:
#   - type: cron
#     metadata:
#       # Required
#       timezone: America/New_York  # The acceptable values would be a value from the IANA Time Zone Database.
#       start: 0 8 * * 1-5        # Start at 8:00 AM
#       end: 0 17 * * 1-5        # Start at 5:00 PM
#       desiredReplicas: "1"


{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ff447363",
   "metadata": {},
   "source": [
    "# Accessing Huggingface Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "28cc6343",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/ml_dev/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e56a1eb8",
   "metadata": {},
   "source": [
    "### Direct Access\n",
    "Access a model directly (not using a pipeline)\n",
    "\n",
    "GPT2 has 124M models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d3a21df",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    }
   ],
   "source": [
    "# Define tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "# Download model\n",
    "model = AutoModelForCausalLM.from_pretrained(\"gpt2\", pad_token_id=tokenizer.eos_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "280ea2d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "I enjoy walking with my cute dog, but I'm not sure if I'll ever be able to walk with my dog. I'm not sure if I'll ever be able to walk with my dog.\n",
      "\n",
      "I'm not sure\n"
     ]
    }
   ],
   "source": [
    "# Try having the model complete the sentence\n",
    "model_inputs = tokenizer(\"I enjoy walking with my cute dog\", return_tensors='pt')\n",
    "\n",
    "# Generate 40 new tokens\n",
    "greedy_output = model.generate(**model_inputs, max_new_tokens=40)\n",
    "\n",
    "print(\"Output:\\n\" + 100 * \"-\")\n",
    "print(tokenizer.decode(greedy_output[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36ee1afd",
   "metadata": {},
   "source": [
    "### Pipeline\n",
    "HF pipelines are a lovely abstraction for inferencing on models in HF hub.  \n",
    "\n",
    "It bundles together the tokenizer and model and makes inference simple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e8cfb4b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n"
     ]
    }
   ],
   "source": [
    "# use pipelines\n",
    "from transformers import pipeline\n",
    "\n",
    "pipe = pipeline(\"text-generation\", model=\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "43e4cbed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'generated_text': 'I enjoy walking with my cute dog, Paws,\" her mother said.\\n\\nThe family also has no plans for another dog who is due to'}]\n"
     ]
    }
   ],
   "source": [
    "# Sentence completion\n",
    "gen1 = pipe(\"I enjoy walking with my cute dog\", truncation=True, max_length=30, num_return_sequences=1)\n",
    "print(gen1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "010fb766",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'generated_text': \"I enjoy walking with my cute dog!\\n\\nPlease don't do this if he thinks you're a big girl, but she's smart enough to\"}, {'generated_text': \"I enjoy walking with my cute dog like one of a kind, but I can't seem to get enough food from the stove! She even has a\"}, {'generated_text': 'I enjoy walking with my cute dog at the park â€“ my favorite part of this trip is when that little one makes a move and we meet up with'}, {'generated_text': \"I enjoy walking with my cute dog.\\n\\n\\nBETONIC (from the German word 'bastigat') is the German term for\"}, {'generated_text': 'I enjoy walking with my cute dog and just wanting to say thanks,\" she says.'}]\n"
     ]
    }
   ],
   "source": [
    "# Sentence completion\n",
    "gen2 = pipe(\"I enjoy walking with my cute dog\", truncation=True, max_length=30, num_return_sequences=5)\n",
    "print(gen2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

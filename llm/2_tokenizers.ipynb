{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "be4d2172",
   "metadata": {},
   "source": [
    "# How to Use Transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11853475",
   "metadata": {},
   "source": [
    "## Tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9732695a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e9293901",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize tokenizer\n",
    "model = \"sshleifer/distilbart-cnn-12-6\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6e9aab4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[    0, 35166,  2239,    16,  2422, 13206,   328,     2],\n",
      "        [    0, 35166,  2239,    16,  1256, 31411,     2,     1]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 0]])}\n"
     ]
    }
   ],
   "source": [
    "# Create inputs\n",
    "raw_inputs = [\"Deep learning is super rad!\", \"Deep learning is pretty lame\"]\n",
    "inputs = tokenizer(raw_inputs, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "print(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "57005d95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer output for: Deep learning is super rad!\n",
      "Input ids: tensor([    0, 35166,  2239,    16,  2422, 13206,   328,     2])\n",
      "Attention Mask: tensor([1, 1, 1, 1, 1, 1, 1, 1])\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Tokenizer output for Deep learning is pretty lame\n",
      "Input ids: tensor([    0, 35166,  2239,    16,  1256, 31411,     2,     1])\n",
      "Attention Mask: tensor([1, 1, 1, 1, 1, 1, 1, 0])\n"
     ]
    }
   ],
   "source": [
    "# Run tokenizer on inputs\n",
    "print(f'Tokenizer output for: {raw_inputs[0]}')\n",
    "print(f\"Input ids: {inputs['input_ids'][0]}\")\n",
    "print(f\"Attention Mask: {inputs['attention_mask'][0]}\")\n",
    "print(\"-\"*100)\n",
    "print(f'Tokenizer output for {raw_inputs[1]}')\n",
    "print(f\"Input ids: {inputs['input_ids'][1]}\")\n",
    "print(f\"Attention Mask: {inputs['attention_mask'][1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4b47984",
   "metadata": {},
   "source": [
    "Padding tokens are used to normalize the length of the inputs.  The second input \"Deep learning is the pits\" has one fewer token than \"Deep learning is super rad!\"  This is because the exclamation point is counted as having semantic meaning.  The tokenizer can automatically adjust the lengths to be the same.  The Attention Mask literally tells the model which tokens to pay attention to.  A 1 represents \"True\" or \"do pay attention\" and a 0 represents \"False\" or \"do not pay attention\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4c5aee9",
   "metadata": {},
   "source": [
    "### Tokenizers Under the Hood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f0488d3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Deep', 'Ġlearning', 'Ġis', 'Ġsuper', 'Ġrad', '!']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# View individual tokens\n",
    "tokens = tokenizer.tokenize(f\"{raw_inputs[0]}\")\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "468b8003",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[35166, 2239, 16, 2422, 13206, 328]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# View token ids\n",
    "token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "token_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e6088bc",
   "metadata": {},
   "source": [
    "Notice how the token_ids are *almost* the same as the token_ids in the inputs above.  When the tokens are being formatted as inputs for a model, the model puts in a \"0\" to indicate the beginning of a sentence, and a \"2\" to indicate the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cbae7f0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [0, 35166, 2239, 16, 2422, 13206, 328, 2], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the inputs ready for the model\n",
    "model_prepped_ids = tokenizer.prepare_for_model(token_ids)\n",
    "model_prepped_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7e83ce60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Deep learning is super rad!'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Decode the token_ids\n",
    "decoded_tokens = tokenizer.decode(token_ids)\n",
    "decoded_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "788ff569",
   "metadata": {},
   "source": [
    "## Pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "026b05f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d736b77a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision 714eb0f (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Device set to use mps:0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'label': 'POSITIVE', 'score': 0.984260618686676},\n",
       " {'label': 'NEGATIVE', 'score': 0.9996439218521118}]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a classifier pipeline\n",
    "classifier = pipeline(\"sentiment-analysis\")\n",
    "classifier(raw_inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59cea8e4",
   "metadata": {},
   "source": [
    "Even if you don't specify a model to use, it will default to one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a8322daf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to openai-community/gpt2 and revision 607a30d (https://huggingface.co/openai-community/gpt2).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e828cf85cd2e4bfca8dac8b3c100ed1c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4427d06669e44f63838b0d05d753e54a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c26f7fbbec84c2eaa4c6be175f415df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad79f0cdaf9d448fad18d19b82d81abe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc2eebbe3f0a490dbf3afb8ebec762d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d3ff3b6f45d4cf2b419bfa0a15a045d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4475f758450402f9545b21496cdfdb5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[{'generated_text': 'I went to the store to buy a cup of ice cream,\" says Joe. \"Everybody asked me why I didn\\'t dress earlier or on Halloween. I was scared of the thought that I\\'d have to wear the same color, the same size,'}],\n",
       " [{'generated_text': 'When two objects in space get close together and share the same color to create a background around each other I use the same method defined by the CSS class.\\n\\nYou can see the two methods defined in the middle of the CSS class definition:\\n'}]]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a text-generation pipeline\n",
    "text_generator = pipeline(\"text-generation\")\n",
    "text_generator([\n",
    "    \"I went to the store to buy a\",\n",
    "    \"When two objects in space get close together\"\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ef92ee4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to sshleifer/distilbart-cnn-12-6 and revision a4f8f3e (https://huggingface.co/sshleifer/distilbart-cnn-12-6).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71b7ae973ba140f0ae2b1fcb93f25777",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/1.22G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c764bdb3ce44f3f899afdab0526dbe7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.22G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[{'summary_text': ' A Fibonacci heap is a collection of trees satisfying the min-heap property . It allows faster amortized time for many operations than binary or binomial heaps . Nodes have a \"mark\" indicating if they\\'ve lost a child since the last time they were made a child of another node .'}]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a summarization pipeline\n",
    "summarizer = pipeline(\"summarization\")\n",
    "summarizer([\n",
    "    \"\"\"A Fibonacci heap is a collection of trees satisfying the min-heap property. It allows faster amortized time for many operations than binary or binomial heaps.\n",
    "    Trees in a Fibonacci heap can have any shape, which facilitates efficient operations. Lazy strategies are employed: node removals and consolidations are delayed until\n",
    "    absolutely necessary (like during an extract-min operation). The main advantage lies in decreasing a key and merging two heaps, which are constant and amortized\n",
    "    constant time, respectively. Nodes have a \"mark\" indicating if they've lost a child since the last time they were made a child of another node, assisting in\n",
    "    restructuring during operations.\"\"\"\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43acd757",
   "metadata": {},
   "source": [
    "## Directly Accessing Pretrained models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2e6e96a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd9cb7ee16d74e64af84188953bef7e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6453e86d3ae4c61b960eeef1fe0040a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/629 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0e8434153f4439da7f47255762c51ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e962986cbe44cb1b36fa4250f155bb3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load model directly\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased-finetuned-sst-2-english\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"distilbert-base-uncased-finetuned-sst-2-english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f8e4ec8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[ 101, 1045, 2293, 2784, 4083,  102]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# View inputs\n",
    "inputs = tokenizer('I love deep learning', return_tensors='pt')\n",
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23463025",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SequenceClassifierOutput(loss=None, logits=tensor([[-4.1975,  4.4937]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# View model outputs\n",
    "outputs = model(**inputs)\n",
    "outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a17d0a6",
   "metadata": {},
   "source": [
    "## Model Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "28f69d56",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "cf6b16c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model\n",
    "model = AutoModel.from_pretrained(\"distilbert-base-uncased-finetuned-sst-2-english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f2944c7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 7, 768])\n"
     ]
    }
   ],
   "source": [
    "# View outputs from model\n",
    "inputs = tokenizer('I love deep learning!', padding=True, truncation=True, return_tensors='pt')\n",
    "\n",
    "outputs = model(**inputs)\n",
    "\n",
    "print(outputs.last_hidden_state.shape) # the token embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9e739ffd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 768])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# to get the full context vector for the sequence\n",
    "context_vectors = outputs.last_hidden_state.mean(dim=1)\n",
    "context_vectors.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "055fb0e5",
   "metadata": {},
   "source": [
    "## Accessing Model Config & Creating Custom Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "2194949d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2Config, GPT2Model\n",
    "\n",
    "# Building the config\n",
    "config = GPT2Config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a6053125",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT2Config {\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 12,\n",
      "  \"n_positions\": 1024,\n",
      "  \"reorder_and_upcast_attn\": false,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"scale_attn_by_inverse_layer_idx\": false,\n",
      "  \"scale_attn_weights\": true,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50257\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "5527e0b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building the model from the config\n",
    "gpt_model = GPT2Model(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "032687ed",
   "metadata": {},
   "source": [
    "The cell above creates a model with activations and randomly initialized weights.  It's untrained, so it's not going to produce any meaningful output.  But, the config is what defines all of the parameters for this new model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76efb118",
   "metadata": {},
   "source": [
    "## Save Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b1372ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "diredctory_name = 'gpt2_model'\n",
    "gpt_model.save_pretrained(directory_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a48377fb",
   "metadata": {},
   "source": [
    "This will save your model as at least a `config.json` and a `model.safetensors` file in a new directory within your PWD."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "undefined.undefined.undefined"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

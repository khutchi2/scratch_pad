{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "effdec19",
   "metadata": {},
   "source": [
    "# Model Metrics Report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bceded97",
   "metadata": {},
   "source": [
    "## Pull Logs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6bedcc3",
   "metadata": {},
   "source": [
    "### Setup S3 Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4453e95c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Automate log pull\n",
    "import boto3\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import random\n",
    "import numpy as np\n",
    "import tarfile\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "301ac352",
   "metadata": {},
   "source": [
    "### Download Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d2ece4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_s3_folder(bucket_name: str, prefix: str, local_dir: str):\n",
    "    \"\"\"\n",
    "    Download all objects under `prefix` in `bucket_name` to `local_dir`,\n",
    "    preserving the S3 “folder” structure.\n",
    "    \"\"\"\n",
    "    \n",
    "    paginator = s3.get_paginator('list_objects_v2')\n",
    "\n",
    "    # Ensure local_dir exists\n",
    "    os.makedirs(local_dir, exist_ok=True)\n",
    "\n",
    "    for page in paginator.paginate(Bucket=bucket_name, Prefix=prefix):\n",
    "        for obj in page.get('Contents', []):\n",
    "            key = obj['Key']\n",
    "            # Strip the prefix from the key, so we recreate only the sub-dirs\n",
    "            relative_path = os.path.relpath(key, prefix)\n",
    "            # Build the full local path\n",
    "            local_path = os.path.join(local_dir, relative_path)\n",
    "\n",
    "            # If the key ends with '/', it’s a “folder” placeholder – skip it\n",
    "            if key.endswith('/'):\n",
    "                continue\n",
    "\n",
    "            # Make sure local subdirectory exists\n",
    "            os.makedirs(os.path.dirname(local_path), exist_ok=True)\n",
    "\n",
    "            # Download the file\n",
    "            s3.download_file(bucket_name, key, local_path)\n",
    "            print(f\"Downloaded s3://{bucket_name}/{key} → {local_path}\")\n",
    "    \n",
    "    print(f\"All files for {bucket_name} downloaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "435ac63d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# AWS Keys\n",
    "s3 = boto3.client('s3', aws_access_key_id='ACCESS_KEY', aws_secret_access_key='SECRET_ACCESS_KEY')\n",
    "\n",
    "# Names\n",
    "BUCKET = \"BUCKET_NAME\"\n",
    "LLM_NAME = \"LLM_NAME\"\n",
    "PREFIX = f\"{LLM_NAME}/\"    # trailing slash ensures we only get that “folder”\n",
    "LOCAL_DIR = f\"./benchmarks/{LLM_NAME}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77f04eff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download subdirectory\n",
    "download_s3_folder(BUCKET, PREFIX, LOCAL_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cbc6cb9",
   "metadata": {},
   "source": [
    "### Unpack Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1792faf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make directries for archive files\n",
    "prometheus_logs_dir = f\"{LOCAL_DIR}/extracted/prometheus_logs\"\n",
    "proxy_logs_dir = f\"{LOCAL_DIR}/extracted/proxy_logs\"\n",
    "\n",
    "os.makedirs(prometheus_logs_dir, exist_ok=True)\n",
    "os.makedirs(proxy_logs_dir, exist_ok=True)\n",
    "\n",
    "# Unarchive into directories\n",
    "with tarfile.open(f\"{LOCAL_DIR}/tests/prometheus_logs.tar\", \"r:tar\") as tar:\n",
    "    tar.extractall(prometheus_logs_dir)\n",
    "with tarfile.open(f\"{LOCAL_DIR}/tests/proxy_logs.tar.gz\", \"r:gz\") as tar:\n",
    "    tar.extractall(proxy_logs_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "626f23e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert tsdb --> parquet\n",
    "chunks_dir_list = os.listdir(prometheus_logs_dir)\n",
    "chunks_dir_list = [dir for dir in chunks_dir_list if len(dir)>=26]\n",
    "\n",
    "# Create Output Dir\n",
    "output_dir = f\"{LOCAL_DIR}/parquet\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Run prometheus tsdb dump cli tool\n",
    "index = 0\n",
    "for dir in chunks_dir_list:\n",
    "    path = f\"{prometheus_logs_dir}/{dir}/\"\n",
    "    output_path = f\"{output_dir}/{index}_\"\n",
    "    !prometheus-tsdb-dump --block $path --output $output_path\n",
    "    index += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15eb3bba",
   "metadata": {},
   "source": [
    "## Compute Resource"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b6aab79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of parquet files\n",
    "parquet_path_list = os.listdir(output_dir)\n",
    "parquet_path_list = [f\"{output_dir}/{file}\" for file in parquet_path_list]\n",
    "parquet_path_list.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7badeebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read parquet files into df\n",
    "columns_list = [\"Labels\", \"MinTimestamp\", \"MaxTimestamp\", \"MinValue\", \"MaxValue\"]\n",
    "full_df = pd.concat( pd.read_parquet(parquet_file, columns=columns_list) for parquet_file in parquet_path_list )\n",
    "full_df[\"Labels\"] = full_df[\"Labels\"].astype(str)\n",
    "full_df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2877d10c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Peak Memory Footprint\n",
    "# Should be able to find peak memory by looking for: go_memstats_alloc_bytes\n",
    "memory_df = full_df.loc[full_df.Labels.str.contains(r\".*go_memstats_alloc_bytes.*\")]\n",
    "PEAK_RAM_GB = memory_df[\"MaxValue\"].max()/1000000000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50a1a519",
   "metadata": {},
   "outputs": [],
   "source": [
    " # Hardware used for benchmarking\n",
    "try:\n",
    "    import json\n",
    "    with open(f\"{LOCAL_DIR}/final/ticket.json\", 'r', encoding='utf-8') as f:\n",
    "        ticket_json = json.load(f)\n",
    "\n",
    "    # Pull information about instance\n",
    "    instance_info = ticket_json['instance']\n",
    "    INSTANCE_TYPE = instance_info['Instance Type']\n",
    "    RAM = instance_info['RAM']\n",
    "    NUM_GPU = instance_info['GPU Count']\n",
    "    GPU_TYPE = instance_info['GPU Type']\n",
    "    VRAM_TOTAL = instance_info['GPU RAM SUM']\n",
    "    COST_PER_HR_USD = instance_info['Price']\n",
    "except:\n",
    "    instance_info = INSTANCE_TYPE = RAM = NUM_GPU = GPU_TYPE = VRAM_TOTAL = COST_PER_HR_USD = \"Not Available\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c192a35",
   "metadata": {},
   "source": [
    "## Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9cfe94f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Proxy Logs Request Response List\n",
    "proxy_logs_dir = f\"{LOCAL_DIR}/extracted/proxy_logs\"\n",
    "list_of_proxy_logs = os.listdir(proxy_logs_dir)\n",
    "pattern = re.compile(r\".*\\:\\d+\")\n",
    "list_of_names = [pattern.search(log).group() for log in list_of_proxy_logs]\n",
    "list_of_names = list(set(list_of_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58f2f01a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate mean tokens/sec\n",
    "tokens_per_second_array = np.array([])\n",
    "processing_times = np.array([])\n",
    "\n",
    "for log in sample:\n",
    "    # Read request\n",
    "    with open(f\"{proxy_logs_dir}/{log}.request\", \"r\") as file:\n",
    "        lines = file.readlines()\n",
    "        pattern = re.compile(\"content-length.*\")\n",
    "        match = list(filter(pattern.match, lines))\n",
    "        request_tokens = int(match[0].strip().split(\" \")[1])/4\n",
    "\n",
    "    # Read response\n",
    "    with open(f\"{proxy_logs_dir}/{log}.response\", \"r\") as file:\n",
    "        lines = file.readlines()\n",
    "        pattern = re.compile(\"content-length.*\")\n",
    "        match = list(filter(pattern.match, lines))\n",
    "        response_tokens = int(match[0].strip().split(\" \")[1])/4\n",
    "\n",
    "        pattern = re.compile(\"X-LAS-Proxy-ProcessingTimeMS.*\")\n",
    "        match = list(filter(pattern.match, lines))\n",
    "        processing_time_seconds = int(match[0].strip().split(\" \")[1])/1000\n",
    "        \n",
    "    processing_times = np.append(processing_times, [processing_time_seconds])\n",
    "    tokens_per_sec = (request_tokens + response_tokens)/processing_time_seconds\n",
    "    tokens_per_second_array = np.append(tokens_per_second_array, [tokens_per_sec])\n",
    "\n",
    "# Calculate mean tokens/sec\n",
    "mean_tokens_per_second = tokens_per_second_array.mean()\n",
    "\n",
    "# Calculate mean processing time sec/query\n",
    "mean_processing_time = processing_times.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec44673c",
   "metadata": {},
   "source": [
    "## Cloud Compute Cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "936348c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cost per million tokens\n",
    "hours_to_1M_tokens = 1000000/(mean_tokens_per_second*3600)\n",
    "cost_per_million_tokens = hours_to_1M_tokens*COST_PER_HR_USD\n",
    "print(cost_per_million_tokens)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
